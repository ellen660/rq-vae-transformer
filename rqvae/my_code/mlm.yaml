#TODO: generate two steps ahead 
#loss twice 

dataset:
  cv: 0
  max_length: 840
  batch_size: 32
  num_workers: 10
  mgh: 0.
  shhs2: 1.
  shhs1: 0.
  mros1: 0.
  mros2: 0.
  wsc: 0.
  cfs: 0.
  bwh: 0.
  mesa: 0.
  debug: False
  thorax: 1.
  abdominal: 0.
  masking: True

arch:
  type: rq-transformer
  block_size: [ 840, 32 ]
  ema: False
  num_steps: 4

  embed_dim: 256
  input_embed_dim: 256
  shared_tok_emb: true
  shared_cls_emb: true

  input_emb_vqvae: False
  head_emb_vqvae: False
  cumsum_depth_ctx: true

  vocab_size_cond: 1 #SOS
  block_size_cond: 1
  vocab_size: 513 #number of bins

  use_spatial_masking: False #if false, then bidrectional

  embd_pdrop: 0.0 #default 0.0, 

  body:
    n_layer: 12 #16 body layers  #default 12 #24k/48k for 16
    block:
      n_head: 16 #32 heads  #default 16  #for 8 20k, for 32 23k
      embed_dim: 256
  head:
    n_layer: 12 #8 layers  #default 4 #32k for 8 layers
    block:
      n_head: 16 #32 heads  #default 16 #36k for 32
      embed_dim: 256

loss:
  type: soft_target_cross_entropy
  soft: False
  stochastic_codes: False
  temp: 0.5 #Try 0.1

optimizer: #warmup
  type: adamW
  init_lr: 0.0005 #learning rate
  weight_decay: 0.0001
  betas: [0.9, 0.95]
  warmup: #add warmup 
    epoch: 15
    multiplier: 1
    buffer_epoch: 0
    min_lr: 0.0
    mode: fix
    start_from_zero: True
  max_gn: 1.0 

common:
  seed: 42
  amp: False
  max_epoch: 1000
  save_ckpt_freq: 10
  test_freq: 10
  sample:
    top_k: 16384
    top_p: 0.92
